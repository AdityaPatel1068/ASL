# ASLCS - American Sign Language Classification System
Enhancing Communication Through Machine Learning and Computer Vision


### Overview

American Sign Language (ASL) is a sophisticated and expressive visual-gestural language primarily employed by the Deaf and hard-of-hearing communities in the United States of America. Successful communication in ASL relies on a diverse array of hand forms, facial expressions, and body movements. However, comprehending and interpreting these signs can pose a challenge for those not proficient in ASL. This is where the integration of machine learning (ML) and computer vision techniques becomes pivotal.

The multiclass classification of American Sign Language using machine learning represents an engaging and practical application of artificial intelligence. The objective is to address the communication gap between the Deaf community and the hearing world. This project entails training a machine learning model to recognize and interpret the various signs and gestures employed in ASL. The primary goal is to facilitate the automatic translation of sign language into text or spoken language, thereby enhancing accessibility and inclusivity for individuals unfamiliar with ASL.

### Motivation
While the Deaf and Dumb community in first-world countries benefits from substantial resources and supportive infrastructure, their counterparts in third-world countries often grapple with a lack of essential communication infrastructure. In countries like India, the absence of proper means of communication beyond basic sign language is palpable.

It is imperative that the voices of Deaf and Dumb individuals resonate on a global scale, transcending geographical boundaries. They merit overdue access to education and opportunities that the rest of the world readily enjoys. While this project may not represent a monumental leap, it serves as a pivotal stepping stone towards fostering inclusivity and understanding for a community that deserves widespread recognition.
### Problem Statement

At present, 18% of the population encounters difficulties in hearing. Our project seeks to develop an artificial voice for American Sign Language, leveraging ASL images as input to generate audio output. This initiative aims to surmount communication barriers and contribute to a more inclusive and accessible environment for individuals facing hearing challenges.
